{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DDPG_TD3.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WXu1r8qvSzWf"},"source":["# Deep Deterministic Policy Gradient (TD3) Twin Delayed."]},{"cell_type":"markdown","metadata":{"id":"YRzQUhuUTc0J"},"source":["## Packages install\n"]},{"cell_type":"code","metadata":{"id":"HAHMB0Ze8fU0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611385279142,"user_tz":-60,"elapsed":14832,"user":{"displayName":"Fran Vaquer","photoUrl":"","userId":"05040906435658654965"}},"outputId":"4c511333-ddff-41a6-8766-0fab18a388ea"},"source":["!pip install pybullet"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting pybullet\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/39/c56526c130f092d0123c471c1a749edf45cb74e97b4cdf6a5230a0ce4054/pybullet-3.0.8-cp36-cp36m-manylinux1_x86_64.whl (76.6MB)\n","\u001b[K     |████████████████████████████████| 76.6MB 38kB/s \n","\u001b[?25hInstalling collected packages: pybullet\n","Successfully installed pybullet-3.0.8\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xjm2onHdT-Av"},"source":["## Importar las librerías"]},{"cell_type":"code","metadata":{"id":"Ikr2p0Js8iB4","executionInfo":{"status":"ok","timestamp":1611385283732,"user_tz":-60,"elapsed":11377,"user":{"displayName":"Fran Vaquer","photoUrl":"","userId":"05040906435658654965"}}},"source":["import os\n","import time\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pybullet_envs\n","import gym\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from gym import wrappers\n","from torch.autograd import Variable\n","from collections import deque"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c3_VY95y-eIi"},"source":["## 1. Experiences repeat memory initialization"]},{"cell_type":"code","metadata":{"id":"I6gUmorj310e","executionInfo":{"status":"ok","timestamp":1611385283735,"user_tz":-60,"elapsed":10647,"user":{"displayName":"Fran Vaquer","photoUrl":"","userId":"05040906435658654965"}}},"source":["class ReplayBuffer(object):\n","  def __init__(self, max_size = 1e6):\n","    self.storage = []\n","    self.max_size = max_size\n","    self.ptr = 0\n","  def add(self, transition):\n","    if len(self.storage) == self.max_size:\n","      self.storage[int(self.ptr)] == transition\n","      self.ptr = (self.ptr + 1) % self.max_size\n","    else:\n","      self.storage.append(transition)\n","  def sample(self, batch_size):\n","    ind = np.random.randint(0, len(self.storage), size = batch_size)\n","    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n","    for i in ind:\n","      state, next_state, action, reward, done = self.storage[i]\n","      batch_states.append(np.array(state, copy= False))\n","      batch_next_states.append(np.array(next_state, copy= False))\n","      batch_actions.append(np.array(action, copy= False))\n","      batch_rewards.append(np.array(reward, copy = False))\n","      batch_dones.append(np.array(done, copy= False))\n","    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1,1), np.array(batch_dones).reshape(-1,1)\n","    \n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i5dGqiJp-27r"},"source":["## 2. Build neural network to model actor and a neural network to target actor"]},{"cell_type":"code","metadata":{"id":"-h0rfmFy-b38","executionInfo":{"status":"ok","timestamp":1611385283735,"user_tz":-60,"elapsed":9531,"user":{"displayName":"Fran Vaquer","photoUrl":"","userId":"05040906435658654965"}}},"source":["class Actor(nn.Module):\n","  def __init__(self, state_dim, action_dim, max_action):\n","    super(Actor, self).__init__()\n","    self.layer_1 = nn.Linear(in_features= state_dim, out_features= 400)\n","    self.layer_2 = nn.Linear(400, 300)\n","    self.layer_3 = nn.Linear(300, action_dim)\n","    self.max_action = max_action\n","\n","  def forward(self, x):\n","    x = F.relu(self.layer_1(x))\n","    x = F.relu(self.layer_2(x))\n","    x = self.max_action * torch.tanh(self.layer_3(x))\n","    return  x\n","    "],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I7cpjaBgD7eJ"},"source":["## 3. Build neural network to two both model critics and a neural network to two both target critics"]},{"cell_type":"code","metadata":{"id":"G6zTCyb_D48X","executionInfo":{"status":"ok","timestamp":1611385283736,"user_tz":-60,"elapsed":9035,"user":{"displayName":"Fran Vaquer","photoUrl":"","userId":"05040906435658654965"}}},"source":["class Critic(nn.Module):\n","  def __init__(self, state_dim, action_dim):\n","    super(Critic, self).__init__()\n","\n","    #Define twin 1 as a deep neural network\n","    self.layer_1 = nn.Linear(in_features= state_dim + action_dim, out_features= 400)\n","    self.layer_2 = nn.Linear(400, 300)\n","    self.layer_3 = nn.Linear(300, 1)\n","\n","    #Define twin 2 as a deep neural network\n","    self.layer_4 = nn.Linear(in_features= state_dim + action_dim, out_features= 400)\n","    self.layer_5 = nn.Linear(400, 300)\n","    self.layer_6 = nn.Linear(300, 1)\n","\n","  def forward(self, x, u):\n","    xu = torch.cat([x,u], 1) #Vertical concat\n","\n","    #Forward propagation of first critic\n","    x1 = F.relu(self.layer_1(xu))\n","    x1 = F.relu(self.layer_2(x1))\n","    x1 = self.layer_3(x1)\n","\n","    #Forward propagation of second critic\n","    x2 = F.relu(self.layer_4(xu))\n","    x2 = F.relu(self.layer_5(x2))\n","    x2 = self.layer_6(x2)\n","\n","    return  x1, x2"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dH3XE33JH83a"},"source":["## 4 - 15. Training"]},{"cell_type":"code","metadata":{"id":"VHqZDnoCH71o","executionInfo":{"status":"ok","timestamp":1611385283736,"user_tz":-60,"elapsed":8536,"user":{"displayName":"Fran Vaquer","photoUrl":"","userId":"05040906435658654965"}}},"source":["#Select GPU or CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","#Build training process in one class\n","\n","class TD3(object):\n","  def __init__(self, state_dim, action_dim, max_action):\n","    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n","    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n","    self.actor_target.load_state_dict(self.actor.state_dict())\n","    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n","    self.critic = Critic(state_dim, action_dim).to(device)\n","    self.critic_target = Critic(state_dim, action_dim).to(device)\n","    self.critic_target.load_state_dict(self.critic.state_dict())\n","    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n","    self.max_action = max_action\n","\n","  def select_action(self, state):\n","    state = torch.Tensor(state.reshape(1, -1)).to(device)\n","    return self.actor(state).cpu().data.numpy().flatten()\n","\n","  def train(self, replay_buffer, iterations, batch_size = 100, discount = 0.99, tau = 0.005, policy_noise = 0.2, noise_clipping = 0.5, policy_freq = 2):\n","    for it in range(iterations):\n","      #Take a sample from state, next state, action and reward of the memory. \n","      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n","      state = torch.Tensor(batch_states).to(device)\n","      next_state = torch.Tensor(batch_next_states).to(device)\n","      action = torch.Tensor(batch_actions).to(device)\n","      reward = torch.Tensor(batch_rewards).to(device)\n","      done = torch.Tensor(batch_dones).to(device)\n","      #For each element of sample:\n","      #from next state, the actor target execute next action\n","      next_action = self.actor_target(next_state).to(device)\n","      #Add gaussian noise to the next action and cut to have in range of accepted environment\n","      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n","      noise = noise.clamp(-noise_clipping, noise_clipping)\n","      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n","      #both target critics take next action and next state for inputs and return two Q-values for outputs\n","      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n","      #take the minimum of the values that are obtained before\n","      target_Q = torch.min(target_Q1, target_Q2)\n","      #Get the final Q target from the two critic models Q = r + y* min(Q11,Q12), where y is a discount factor\n","      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n","      #Two critics take state and action for inputs and return two Q-values Q1 and Q2 for output\n","      current_Q1, current_Q2 = self.critic(state, action)\n","      #Calculate loss from model critic: Critic_loss = MSE_Loss(Q1(s, a), Qt) + MSE_Loss(Q2(s, a), Qt)\n","      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n","      #Back propagation of the mse_loss and upgrade the two parameters of critic model with SGD\n","      self.critic_optimizer.zero_grad()\n","      critic_loss.backward()\n","      self.critic_optimizer.step()\n","      #To each of two iterations, upgrade the actor model by play the ascendence gradient at the first model critic output\n","      if it%policy_freq == 0:\n","        actor_loss = - self.critic(state, self.actor(state))[0].mean() #Acendence gradient from negative descendence gradient / REVISAR PASO 15\n","        self.actor_optimizer.zero_grad()\n","        actor_loss.backward()\n","        self.actor_optimizer.step()\n","        #Upgrade the weights of the target actor using polyak model\n","        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n","          target_param.data.copy_(tau * param + (1 - tau) * target_param)\n","        #Upgrade the weights of the target critic using polyak model\n","        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n","          target_param.data.copy_(tau * param + (1 - tau) * target_param)\n","    #Method to save the trained model\n","  def save(self, filename, directory):\n","      torch.save(self.actor.state_dict(), \"%s/%s_actor.pth\" % (directory, filename))\n","      torch.save(self.critic.state_dict(), \"%s/%s_critic.pth\" % (directory, filename))\n","    #Method to load the trained model\n","  def load(self, filename, directory):\n","      self.actor.load_state_dict(torch.load(\"%s/%s_actor.pth\" % (directory, filename)))\n","      self.critic.load_state_dict(torch.load(\"%s/%s_critic.pth\" % (directory, filename)))\n","\n","\n","\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VjWeKmUU4_gJ"},"source":["## Implement a function that evaluate the policy calculating the mean of rewards by 10 episodes"]},{"cell_type":"code","metadata":{"id":"x4NrIdlNpqRd","executionInfo":{"status":"ok","timestamp":1611385283736,"user_tz":-60,"elapsed":6624,"user":{"displayName":"Fran Vaquer","photoUrl":"","userId":"05040906435658654965"}}},"source":["def evaluate_policy(policy, eval_episodes=10):\n","  avg_reward = 0.\n","  for _ in range(eval_episodes):\n","    obs = env.reset()\n","    done = False\n","    while not done:\n","      action = policy.select_action(np.array(obs))\n","      obs, reward, done, _ = env.step(action)\n","      avg_reward += reward\n","  avg_reward /= eval_episodes\n","  print(\"----------------------------------------\")\n","  print(\"Mean reward in the evaluation step: %f\" % (avg_reward))\n","  print(\"----------------------------------------\")\n","  return avg_reward\n","  "],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LVTlWJz36iDf"},"source":["## Configuration of parameters"]},{"cell_type":"code","metadata":{"id":"eoEb5aMO4twY","executionInfo":{"status":"ok","timestamp":1611385283737,"user_tz":-60,"elapsed":5523,"user":{"displayName":"Fran Vaquer","photoUrl":"","userId":"05040906435658654965"}}},"source":["env_name = \"AntBulletEnv-v0\" # Environment name\n","seed = 0 #Value of random seed\n","start_timesteps = 1e4 #Number of iterations during each of them the model choose an random action\n","eval_freq = 5e3 #frecuency of evaluation step\n","max_timesteps = 5e5 #Total number of iterations\n","save_models = True #Check boolean to know if the model pre-trained is would be saved or not\n","expl_noise = 0.1 #Exploration noise: standard desviation of the gaussian exploration noise\n","batch_size = 100\n","discount = 0.99 #Discount gamma factor, utilized in the total discount reward calculate\n","tau = 0.005 #Actualization ratio of the objectives net\n","policy_noise = 0.2 # Standard desviation of aditing gaussian noise to exploration actions\n","noise_clip = 0.5 #Max gaussian noise value aditingto actions (policy)\n","policy_freq = 2 #Number of iterations to wait before actualization of the policy net"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ztA9HEp9ffi"},"source":["## Create a file name to each of two save models: Actor and Critic"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IcVO6AIQ9ehX","executionInfo":{"status":"ok","timestamp":1611385283737,"user_tz":-60,"elapsed":4270,"user":{"displayName":"Fran Vaquer","photoUrl":"","userId":"05040906435658654965"}},"outputId":"3a45cd7d-8759-44df-969f-1c1084323560"},"source":["file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n","print(\"----------------------------------------\")\n","print(\"Configuration: %s\" % (file_name))\n","print(\"----------------------------------------\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["----------------------------------------\n","Configuration: TD3_AntBulletEnv-v0_0\n","----------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c1SjFBT1-AvY"},"source":["## Create a folder where the trained models will saved"]},{"cell_type":"code","metadata":{"id":"bcLe_mQa-L2i","executionInfo":{"status":"ok","timestamp":1611385283737,"user_tz":-60,"elapsed":3132,"user":{"displayName":"Fran Vaquer","photoUrl":"","userId":"05040906435658654965"}}},"source":["if not os.path.exists(\"./results\"):\n","  os.makedirs(\"./results\")\n","if save_models and not os.path.exists(\"./pytorch_models\"):\n","  os.makedirs(\"./pytorch_models\")"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tFw73vrG-R7o"},"source":["## Create an environment of PyBullet"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wbFY4l1V_Rjd","executionInfo":{"status":"ok","timestamp":1611385283738,"user_tz":-60,"elapsed":1450,"user":{"displayName":"Fran Vaquer","photoUrl":"","userId":"05040906435658654965"}},"outputId":"fb71ec93-bde5-42c2-de4e-7ebe983a6465"},"source":["env = gym.make(env_name)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"0hzzqLHz_UvM"},"source":["## Fix the seeds and obtain the needed information about the select environment states and actions"]},{"cell_type":"code","metadata":{"id":"2fVBxM20_3Gt","executionInfo":{"status":"ok","timestamp":1611385287630,"user_tz":-60,"elapsed":1643,"user":{"displayName":"Fran Vaquer","photoUrl":"","userId":"05040906435658654965"}}},"source":["env.seed(seed)\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.shape[0]\n","max_action = float(env.action_space.high[0])\n"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ElHjYBGt_6hc"},"source":["## Create a policy net (model actor)"]},{"cell_type":"code","metadata":{"id":"awCX---AACOz","executionInfo":{"status":"ok","timestamp":1611385298399,"user_tz":-60,"elapsed":11294,"user":{"displayName":"Fran Vaquer","photoUrl":"","userId":"05040906435658654965"}}},"source":["policy = TD3(state_dim, action_dim, max_action)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h9l5CwV7ACx3"},"source":["## Create the experience repeat memory"]},{"cell_type":"code","metadata":{"id":"2QEsdUOQANNo","executionInfo":{"status":"ok","timestamp":1611385298400,"user_tz":-60,"elapsed":10376,"user":{"displayName":"Fran Vaquer","photoUrl":"","userId":"05040906435658654965"}}},"source":["replay_buffer = ReplayBuffer()"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WYkx2cgeBnxY"},"source":["## Define a list where the evaluation results of 10 episodes are going to save"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GAlfuTXeB1cw","executionInfo":{"status":"ok","timestamp":1611385298627,"user_tz":-60,"elapsed":9721,"user":{"displayName":"Fran Vaquer","photoUrl":"","userId":"05040906435658654965"}},"outputId":"30d85b5f-5e00-4859-ec4d-51482af0e390"},"source":["evaluations = [evaluate_policy(policy)]"],"execution_count":15,"outputs":[{"output_type":"stream","text":["----------------------------------------\n","Mean reward in the evaluation step: 9.804960\n","----------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qvdZsuQ1DXzA"},"source":["## Create a new directory of folders to show the final results (agents video)"]},{"cell_type":"code","metadata":{"id":"1DQCOTaCB6YT","executionInfo":{"status":"ok","timestamp":1611385298627,"user_tz":-60,"elapsed":9039,"user":{"displayName":"Fran Vaquer","photoUrl":"","userId":"05040906435658654965"}}},"source":["def mkdir(base, name):\n","  path = os.path.join(base, name)\n","  if not os.path.exists(path):\n","    os.makedirs(path)\n","  return path\n","work_dir = mkdir('exp', 'brs')\n","monitor_dir = mkdir(work_dir, 'monitor')\n","max_episode_steps = env._max_episode_steps\n","save_env_vid = False\n","if save_env_vid:\n","  env = wrappers.Monitor(env, monitor_dir, force = True)\n","  env.reset()"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HuV7k33mEbFE"},"source":["## Initialize variables"]},{"cell_type":"code","metadata":{"id":"NublO7JXEfUY","executionInfo":{"status":"ok","timestamp":1611385298628,"user_tz":-60,"elapsed":8365,"user":{"displayName":"Fran Vaquer","photoUrl":"","userId":"05040906435658654965"}}},"source":["total_timesteps = 0\n","timesteps_since_eval = 0\n","episode_num = 0\n","done = True\n","t0 = time.time()"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tFZx79UZEg65"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"927VaxjqEiHb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611391133306,"user_tz":-60,"elapsed":5832033,"user":{"displayName":"Fran Vaquer","photoUrl":"","userId":"05040906435658654965"}},"outputId":"d54a7b96-0092-4c5b-90b2-7c740121b322"},"source":["#Initialize fristly bucle with 500000 timesteps\n","while total_timesteps < max_timesteps:\n","\n","  #if the episode has ended\n","  if done:\n","\n","    #if we don't stay in the first iteration, run the training process of the model\n","    if total_timesteps != 0:\n","      print(\"Total Timesteps: {} Episode num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n","      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n","\n","    #evaluate the episode and save  the policy if necesary iteration are passed\n","    if timesteps_since_eval >= eval_freq:\n","      timesteps_since_eval %= eval_freq\n","      evaluations.append(evaluate_policy(policy))\n","      policy.save(file_name, directory = \"./pytorch_models\")\n","      np.save(\"./results/%s\" % (file_name), evaluations)\n","\n","    #When episode train end, reset environment\n","    obs = env.reset()\n","\n","    #Configuration of done value to False\n","    dane = False\n","\n","    #Configuration the episode reward and timestep to zero\n","    episode_reward = 0\n","    episode_timesteps = 0\n","    episode_num += 1\n","\n","  # Before 10000 timesteps, run random actions\n","  if total_timesteps < start_timesteps:\n","    action = env.action_space.sample()\n","  #after 10000 timesteps, change the model\n","  else:\n","    action = policy.select_action(np.array(obs))\n","    #if the explore_noise value is not 0, add noise to action and cut the range\n","    if expl_noise != 0:\n","      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n","  \n","  #Agent run an environment action and reach the next state and one reward\n","  new_obs, reward, done,_ = env.step(action)\n","\n","  #chek if the episode is ended\n","  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n","\n","  #Increment total reward\n","  episode_reward += reward\n","\n","  #store the new transition in the replay buffer\n","  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n","\n","  #Upgrade the state, number episode timestep, total timesteps and the number of steps since the last policy actualization\n","  obs = new_obs\n","  episode_timesteps += 1\n","  total_timesteps += 1\n","  timesteps_since_eval += 1\n","\n","#Add the last policy actualization to the previous evaluation list and save our model\n","evaluations.append(evaluate_policy(policy))\n","if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n","np.save(\"./results/%s\" % (file_name), evaluations)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Total Timesteps: 1000 Episode num: 1 Reward: 479.01462369570976\n","Total Timesteps: 2000 Episode num: 2 Reward: 511.8013945921352\n","Total Timesteps: 3000 Episode num: 3 Reward: 342.17752612482985\n","Total Timesteps: 3636 Episode num: 4 Reward: 278.35681274317875\n","Total Timesteps: 3904 Episode num: 5 Reward: 128.59998217105306\n","Total Timesteps: 4904 Episode num: 6 Reward: 529.640584528355\n","Total Timesteps: 5904 Episode num: 7 Reward: 480.20927893245386\n","----------------------------------------\n","Mean reward in the evaluation step: 100.385933\n","----------------------------------------\n","Total Timesteps: 6904 Episode num: 8 Reward: 481.91313816534637\n","Total Timesteps: 7353 Episode num: 9 Reward: 211.21894645876594\n","Total Timesteps: 8353 Episode num: 10 Reward: 305.81699516075116\n","Total Timesteps: 8550 Episode num: 11 Reward: 90.90798523643461\n","Total Timesteps: 9550 Episode num: 12 Reward: 515.0580299115522\n","Total Timesteps: 10550 Episode num: 13 Reward: 481.93879527458245\n","----------------------------------------\n","Mean reward in the evaluation step: 140.885899\n","----------------------------------------\n","Total Timesteps: 11550 Episode num: 14 Reward: 117.39390780182772\n","Total Timesteps: 12550 Episode num: 15 Reward: 198.195603314328\n","Total Timesteps: 13550 Episode num: 16 Reward: 202.1726464704434\n","Total Timesteps: 14550 Episode num: 17 Reward: 269.56703958302563\n","Total Timesteps: 15550 Episode num: 18 Reward: 112.9816344266145\n","----------------------------------------\n","Mean reward in the evaluation step: 195.267396\n","----------------------------------------\n","Total Timesteps: 16550 Episode num: 19 Reward: 103.61163177274015\n","Total Timesteps: 17550 Episode num: 20 Reward: 507.56224619086845\n","Total Timesteps: 18550 Episode num: 21 Reward: 402.40791726776763\n","Total Timesteps: 19550 Episode num: 22 Reward: 493.67339217035715\n","Total Timesteps: 20550 Episode num: 23 Reward: 387.73484421599545\n","----------------------------------------\n","Mean reward in the evaluation step: 201.159683\n","----------------------------------------\n","Total Timesteps: 21550 Episode num: 24 Reward: 171.8192735369642\n","Total Timesteps: 22550 Episode num: 25 Reward: 275.16951766273576\n","Total Timesteps: 23550 Episode num: 26 Reward: 286.301218958751\n","Total Timesteps: 24550 Episode num: 27 Reward: 386.9212727362592\n","Total Timesteps: 25550 Episode num: 28 Reward: 524.2496289114638\n","----------------------------------------\n","Mean reward in the evaluation step: 326.056438\n","----------------------------------------\n","Total Timesteps: 26550 Episode num: 29 Reward: 380.3015863536152\n","Total Timesteps: 27550 Episode num: 30 Reward: 601.3504665063847\n","Total Timesteps: 28550 Episode num: 31 Reward: 440.23948694599375\n","Total Timesteps: 29550 Episode num: 32 Reward: 332.011590472201\n","Total Timesteps: 30550 Episode num: 33 Reward: 425.3728079149007\n","----------------------------------------\n","Mean reward in the evaluation step: 303.237944\n","----------------------------------------\n","Total Timesteps: 31550 Episode num: 34 Reward: 221.50337887456288\n","Total Timesteps: 32550 Episode num: 35 Reward: 269.3187410021385\n","Total Timesteps: 33550 Episode num: 36 Reward: 257.41196855204606\n","Total Timesteps: 34550 Episode num: 37 Reward: 459.0276525987118\n","Total Timesteps: 34808 Episode num: 38 Reward: 121.37014695704117\n","Total Timesteps: 35808 Episode num: 39 Reward: 358.88372225127654\n","----------------------------------------\n","Mean reward in the evaluation step: 102.913636\n","----------------------------------------\n","Total Timesteps: 36808 Episode num: 40 Reward: 106.64954680844703\n","Total Timesteps: 37808 Episode num: 41 Reward: 101.83888824293541\n","Total Timesteps: 37857 Episode num: 42 Reward: 1.6251442343419145\n","Total Timesteps: 37901 Episode num: 43 Reward: 10.758378756315569\n","Total Timesteps: 37929 Episode num: 44 Reward: 2.7650947823174805\n","Total Timesteps: 37980 Episode num: 45 Reward: 4.116494911508655\n","Total Timesteps: 38195 Episode num: 46 Reward: 86.29459393702368\n","Total Timesteps: 39195 Episode num: 47 Reward: 439.76987182149884\n","Total Timesteps: 40195 Episode num: 48 Reward: 305.3278120968825\n","----------------------------------------\n","Mean reward in the evaluation step: 354.900218\n","----------------------------------------\n","Total Timesteps: 41195 Episode num: 49 Reward: 345.96938661310645\n","Total Timesteps: 42195 Episode num: 50 Reward: 441.76702362969064\n","Total Timesteps: 43195 Episode num: 51 Reward: 382.9901131816785\n","Total Timesteps: 44195 Episode num: 52 Reward: 520.5748334832178\n","Total Timesteps: 45195 Episode num: 53 Reward: 408.39626951465124\n","----------------------------------------\n","Mean reward in the evaluation step: 446.074680\n","----------------------------------------\n","Total Timesteps: 46195 Episode num: 54 Reward: 393.5664495680841\n","Total Timesteps: 47195 Episode num: 55 Reward: 404.1761646564721\n","Total Timesteps: 48195 Episode num: 56 Reward: 362.10334374139364\n","Total Timesteps: 49195 Episode num: 57 Reward: 523.9322948047208\n","Total Timesteps: 50195 Episode num: 58 Reward: 437.94867836145653\n","----------------------------------------\n","Mean reward in the evaluation step: 464.800348\n","----------------------------------------\n","Total Timesteps: 51195 Episode num: 59 Reward: 458.72129988534556\n","Total Timesteps: 52195 Episode num: 60 Reward: 509.9008725385482\n","Total Timesteps: 53195 Episode num: 61 Reward: 327.7949489088908\n","Total Timesteps: 54195 Episode num: 62 Reward: 471.3848800510137\n","Total Timesteps: 55195 Episode num: 63 Reward: 485.6895256869584\n","----------------------------------------\n","Mean reward in the evaluation step: 422.072598\n","----------------------------------------\n","Total Timesteps: 56195 Episode num: 64 Reward: 390.17609408733585\n","Total Timesteps: 57195 Episode num: 65 Reward: 351.9621890268934\n","Total Timesteps: 58195 Episode num: 66 Reward: 227.32564025013204\n","Total Timesteps: 59195 Episode num: 67 Reward: 337.5762987585301\n","Total Timesteps: 60195 Episode num: 68 Reward: 319.77750045334506\n","----------------------------------------\n","Mean reward in the evaluation step: 445.978724\n","----------------------------------------\n","Total Timesteps: 61195 Episode num: 69 Reward: 492.3160332726806\n","Total Timesteps: 62195 Episode num: 70 Reward: 439.5689743681115\n","Total Timesteps: 63195 Episode num: 71 Reward: 509.9396338285121\n","Total Timesteps: 64195 Episode num: 72 Reward: 374.7307235887255\n","Total Timesteps: 65195 Episode num: 73 Reward: 501.79596032173816\n","----------------------------------------\n","Mean reward in the evaluation step: 476.232186\n","----------------------------------------\n","Total Timesteps: 66195 Episode num: 74 Reward: 535.5645680890909\n","Total Timesteps: 67195 Episode num: 75 Reward: 561.1016196491438\n","Total Timesteps: 68195 Episode num: 76 Reward: 559.9027176629388\n","Total Timesteps: 69195 Episode num: 77 Reward: 310.1608563445134\n","Total Timesteps: 70195 Episode num: 78 Reward: 311.82058885863086\n","----------------------------------------\n","Mean reward in the evaluation step: -1.136963\n","----------------------------------------\n","Total Timesteps: 70215 Episode num: 79 Reward: 0.7652803669090584\n","Total Timesteps: 70236 Episode num: 80 Reward: -1.1051058292861062\n","Total Timesteps: 71236 Episode num: 81 Reward: 441.3341465697588\n","Total Timesteps: 71256 Episode num: 82 Reward: 1.1844452375858294\n","Total Timesteps: 71276 Episode num: 83 Reward: 0.4070535946818361\n","Total Timesteps: 72276 Episode num: 84 Reward: 425.797992349299\n","Total Timesteps: 73276 Episode num: 85 Reward: 464.08120897213354\n","Total Timesteps: 74276 Episode num: 86 Reward: 246.71544201388912\n","Total Timesteps: 75276 Episode num: 87 Reward: 306.72295583626425\n","----------------------------------------\n","Mean reward in the evaluation step: 215.203018\n","----------------------------------------\n","Total Timesteps: 76276 Episode num: 88 Reward: 233.5084301787154\n","Total Timesteps: 77276 Episode num: 89 Reward: 331.54146160957674\n","Total Timesteps: 78276 Episode num: 90 Reward: 154.4929221186267\n","Total Timesteps: 79276 Episode num: 91 Reward: 430.0360967615995\n","Total Timesteps: 80276 Episode num: 92 Reward: 587.1569334320161\n","----------------------------------------\n","Mean reward in the evaluation step: 525.835766\n","----------------------------------------\n","Total Timesteps: 81276 Episode num: 93 Reward: 443.2454177170662\n","Total Timesteps: 82276 Episode num: 94 Reward: 552.0964871700382\n","Total Timesteps: 82561 Episode num: 95 Reward: 180.72906088738813\n","Total Timesteps: 83561 Episode num: 96 Reward: 631.1153878983064\n","Total Timesteps: 84561 Episode num: 97 Reward: 637.9891124084409\n","Total Timesteps: 84622 Episode num: 98 Reward: 36.937914232140955\n","Total Timesteps: 85622 Episode num: 99 Reward: 516.4963999479339\n","----------------------------------------\n","Mean reward in the evaluation step: 263.239231\n","----------------------------------------\n","Total Timesteps: 86622 Episode num: 100 Reward: 404.12475343469754\n","Total Timesteps: 87622 Episode num: 101 Reward: 470.64248294423857\n","Total Timesteps: 88622 Episode num: 102 Reward: 536.276684937578\n","Total Timesteps: 89262 Episode num: 103 Reward: 244.1168876559746\n","Total Timesteps: 90037 Episode num: 104 Reward: 255.71673798169314\n","----------------------------------------\n","Mean reward in the evaluation step: 272.671070\n","----------------------------------------\n","Total Timesteps: 90181 Episode num: 105 Reward: 31.893171324461875\n","Total Timesteps: 91181 Episode num: 106 Reward: 419.5302464130484\n","Total Timesteps: 92181 Episode num: 107 Reward: 283.84987435546043\n","Total Timesteps: 93181 Episode num: 108 Reward: 209.37767389260205\n","Total Timesteps: 94181 Episode num: 109 Reward: 615.9338080567695\n","Total Timesteps: 95181 Episode num: 110 Reward: 389.4521057336653\n","----------------------------------------\n","Mean reward in the evaluation step: 541.305664\n","----------------------------------------\n","Total Timesteps: 96181 Episode num: 111 Reward: 440.59520949570083\n","Total Timesteps: 96902 Episode num: 112 Reward: 371.2532397030445\n","Total Timesteps: 97902 Episode num: 113 Reward: 400.68317449537585\n","Total Timesteps: 98902 Episode num: 114 Reward: 582.0575131004522\n","Total Timesteps: 99902 Episode num: 115 Reward: 456.36889396625264\n","Total Timesteps: 100902 Episode num: 116 Reward: 614.6622042660474\n","----------------------------------------\n","Mean reward in the evaluation step: 426.482939\n","----------------------------------------\n","Total Timesteps: 101902 Episode num: 117 Reward: 484.28739961005084\n","Total Timesteps: 102902 Episode num: 118 Reward: 518.8386827205985\n","Total Timesteps: 103902 Episode num: 119 Reward: 487.7015451770276\n","Total Timesteps: 104902 Episode num: 120 Reward: 552.8205565367609\n","Total Timesteps: 105902 Episode num: 121 Reward: 457.2863446839264\n","----------------------------------------\n","Mean reward in the evaluation step: 487.180155\n","----------------------------------------\n","Total Timesteps: 106902 Episode num: 122 Reward: 561.3825513598157\n","Total Timesteps: 107902 Episode num: 123 Reward: 598.4178673152671\n","Total Timesteps: 108902 Episode num: 124 Reward: 478.3452237580371\n","Total Timesteps: 109902 Episode num: 125 Reward: 416.06831556007586\n","Total Timesteps: 110902 Episode num: 126 Reward: 393.7303356442117\n","----------------------------------------\n","Mean reward in the evaluation step: 1.735175\n","----------------------------------------\n","Total Timesteps: 110922 Episode num: 127 Reward: 2.362241850147062\n","Total Timesteps: 110943 Episode num: 128 Reward: 1.4351483016043822\n","Total Timesteps: 110963 Episode num: 129 Reward: 2.0094298603038463\n","Total Timesteps: 110984 Episode num: 130 Reward: 1.9716729302196971\n","Total Timesteps: 111017 Episode num: 131 Reward: 6.691755291812305\n","Total Timesteps: 111037 Episode num: 132 Reward: 2.047219729469657\n","Total Timesteps: 111674 Episode num: 133 Reward: 288.2145823344678\n","Total Timesteps: 111722 Episode num: 134 Reward: 10.643553047395915\n","Total Timesteps: 112722 Episode num: 135 Reward: 266.18875270092485\n","Total Timesteps: 113722 Episode num: 136 Reward: 549.017530769537\n","Total Timesteps: 114722 Episode num: 137 Reward: 251.7661003452145\n","Total Timesteps: 115722 Episode num: 138 Reward: 535.2189309774407\n","----------------------------------------\n","Mean reward in the evaluation step: 465.309506\n","----------------------------------------\n","Total Timesteps: 116722 Episode num: 139 Reward: 399.5354103552276\n","Total Timesteps: 117342 Episode num: 140 Reward: 307.5717050665996\n","Total Timesteps: 117425 Episode num: 141 Reward: 36.58261463817759\n","Total Timesteps: 118425 Episode num: 142 Reward: 366.8679111074642\n","Total Timesteps: 119425 Episode num: 143 Reward: 569.0154055106425\n","Total Timesteps: 120425 Episode num: 144 Reward: 448.63858211911383\n","----------------------------------------\n","Mean reward in the evaluation step: 88.565817\n","----------------------------------------\n","Total Timesteps: 120538 Episode num: 145 Reward: 42.87328793503004\n","Total Timesteps: 120635 Episode num: 146 Reward: 43.783266625210224\n","Total Timesteps: 121635 Episode num: 147 Reward: 417.6997128342104\n","Total Timesteps: 122635 Episode num: 148 Reward: 685.3007060566734\n","Total Timesteps: 123635 Episode num: 149 Reward: 639.4197556917424\n","Total Timesteps: 124635 Episode num: 150 Reward: 633.544800215479\n","Total Timesteps: 125635 Episode num: 151 Reward: 364.77012743293614\n","----------------------------------------\n","Mean reward in the evaluation step: 196.817601\n","----------------------------------------\n","Total Timesteps: 126635 Episode num: 152 Reward: 358.97159243519127\n","Total Timesteps: 127635 Episode num: 153 Reward: 476.68511018830947\n","Total Timesteps: 128635 Episode num: 154 Reward: 449.16716365363055\n","Total Timesteps: 129622 Episode num: 155 Reward: 304.28245107231567\n","Total Timesteps: 130622 Episode num: 156 Reward: 368.8900120496229\n","----------------------------------------\n","Mean reward in the evaluation step: 523.356922\n","----------------------------------------\n","Total Timesteps: 131622 Episode num: 157 Reward: 450.6872879159042\n","Total Timesteps: 132622 Episode num: 158 Reward: 597.525229534362\n","Total Timesteps: 133622 Episode num: 159 Reward: 830.0196680044276\n","Total Timesteps: 134622 Episode num: 160 Reward: 605.7983493654931\n","Total Timesteps: 135622 Episode num: 161 Reward: 727.8205843029227\n","----------------------------------------\n","Mean reward in the evaluation step: 531.965555\n","----------------------------------------\n","Total Timesteps: 136622 Episode num: 162 Reward: 583.7317795443739\n","Total Timesteps: 137622 Episode num: 163 Reward: 641.1659334187312\n","Total Timesteps: 138622 Episode num: 164 Reward: 484.19542315434535\n","Total Timesteps: 139622 Episode num: 165 Reward: 686.5021733526044\n","Total Timesteps: 140622 Episode num: 166 Reward: 610.2468891749622\n","----------------------------------------\n","Mean reward in the evaluation step: 554.643438\n","----------------------------------------\n","Total Timesteps: 141622 Episode num: 167 Reward: 640.799928526825\n","Total Timesteps: 142622 Episode num: 168 Reward: 395.8412573977811\n","Total Timesteps: 143622 Episode num: 169 Reward: 537.0237293129928\n","Total Timesteps: 144622 Episode num: 170 Reward: 433.3518635231958\n","Total Timesteps: 145622 Episode num: 171 Reward: 527.776514548646\n","----------------------------------------\n","Mean reward in the evaluation step: 470.351357\n","----------------------------------------\n","Total Timesteps: 146622 Episode num: 172 Reward: 472.4653703962928\n","Total Timesteps: 147622 Episode num: 173 Reward: 423.6814261418183\n","Total Timesteps: 148622 Episode num: 174 Reward: 500.7369111466774\n","Total Timesteps: 149622 Episode num: 175 Reward: 243.8503918730721\n","Total Timesteps: 150622 Episode num: 176 Reward: 673.2316124655098\n","----------------------------------------\n","Mean reward in the evaluation step: 569.719908\n","----------------------------------------\n","Total Timesteps: 151622 Episode num: 177 Reward: 374.6529312932227\n","Total Timesteps: 152622 Episode num: 178 Reward: 532.3752985093755\n","Total Timesteps: 153622 Episode num: 179 Reward: 720.3816893430244\n","Total Timesteps: 154622 Episode num: 180 Reward: 645.0671487434249\n","Total Timesteps: 155622 Episode num: 181 Reward: 438.8982843432994\n","----------------------------------------\n","Mean reward in the evaluation step: 553.900836\n","----------------------------------------\n","Total Timesteps: 156622 Episode num: 182 Reward: 398.83928173683915\n","Total Timesteps: 157622 Episode num: 183 Reward: 754.4059610908593\n","Total Timesteps: 158622 Episode num: 184 Reward: 630.5671696713528\n","Total Timesteps: 159622 Episode num: 185 Reward: 319.28257627165175\n","Total Timesteps: 160622 Episode num: 186 Reward: 315.12520825727444\n","----------------------------------------\n","Mean reward in the evaluation step: 520.008645\n","----------------------------------------\n","Total Timesteps: 161622 Episode num: 187 Reward: 535.5223061847286\n","Total Timesteps: 161739 Episode num: 188 Reward: 19.15285222983924\n","Total Timesteps: 162739 Episode num: 189 Reward: 277.2668123924263\n","Total Timesteps: 163739 Episode num: 190 Reward: 586.5107509760902\n","Total Timesteps: 164739 Episode num: 191 Reward: 311.390842479438\n","Total Timesteps: 165739 Episode num: 192 Reward: 414.4627288096862\n","----------------------------------------\n","Mean reward in the evaluation step: 462.716327\n","----------------------------------------\n","Total Timesteps: 166739 Episode num: 193 Reward: 487.88023028623473\n","Total Timesteps: 167739 Episode num: 194 Reward: 503.8107276380697\n","Total Timesteps: 168739 Episode num: 195 Reward: 678.8517422969022\n","Total Timesteps: 168929 Episode num: 196 Reward: 120.43159565558057\n","Total Timesteps: 169929 Episode num: 197 Reward: 447.557563642037\n","Total Timesteps: 170929 Episode num: 198 Reward: 698.9509033767624\n","----------------------------------------\n","Mean reward in the evaluation step: 553.539757\n","----------------------------------------\n","Total Timesteps: 171929 Episode num: 199 Reward: 566.2880692755864\n","Total Timesteps: 172929 Episode num: 200 Reward: 585.3469111448602\n","Total Timesteps: 173929 Episode num: 201 Reward: 598.8043532648296\n","Total Timesteps: 174929 Episode num: 202 Reward: 588.8972587540695\n","Total Timesteps: 175929 Episode num: 203 Reward: 446.05045671449693\n","----------------------------------------\n","Mean reward in the evaluation step: 480.183353\n","----------------------------------------\n","Total Timesteps: 176929 Episode num: 204 Reward: 559.8105711831142\n","Total Timesteps: 177929 Episode num: 205 Reward: 620.1976056464097\n","Total Timesteps: 178929 Episode num: 206 Reward: 566.7457254174617\n","Total Timesteps: 179929 Episode num: 207 Reward: 583.1101095961411\n","Total Timesteps: 180929 Episode num: 208 Reward: 504.19753579928266\n","----------------------------------------\n","Mean reward in the evaluation step: 656.758375\n","----------------------------------------\n","Total Timesteps: 181929 Episode num: 209 Reward: 653.17960035084\n","Total Timesteps: 182929 Episode num: 210 Reward: 800.4960150983618\n","Total Timesteps: 183929 Episode num: 211 Reward: 492.126158022124\n","Total Timesteps: 184929 Episode num: 212 Reward: 521.9783855416975\n","Total Timesteps: 185929 Episode num: 213 Reward: 550.9844294285376\n","----------------------------------------\n","Mean reward in the evaluation step: 580.666946\n","----------------------------------------\n","Total Timesteps: 186929 Episode num: 214 Reward: 456.28039250339344\n","Total Timesteps: 187929 Episode num: 215 Reward: 538.3437495253323\n","Total Timesteps: 188929 Episode num: 216 Reward: 717.1312616821051\n","Total Timesteps: 189929 Episode num: 217 Reward: 671.1275750762499\n","Total Timesteps: 190929 Episode num: 218 Reward: 480.135862690812\n","----------------------------------------\n","Mean reward in the evaluation step: 568.946791\n","----------------------------------------\n","Total Timesteps: 191929 Episode num: 219 Reward: 508.19406670311696\n","Total Timesteps: 192929 Episode num: 220 Reward: 590.7121519453485\n","Total Timesteps: 193929 Episode num: 221 Reward: 459.7646753205528\n","Total Timesteps: 194929 Episode num: 222 Reward: 604.4118969751347\n","Total Timesteps: 195929 Episode num: 223 Reward: 784.1977618121625\n","----------------------------------------\n","Mean reward in the evaluation step: 618.068556\n","----------------------------------------\n","Total Timesteps: 196929 Episode num: 224 Reward: 582.814282415867\n","Total Timesteps: 197929 Episode num: 225 Reward: 659.0656479735433\n","Total Timesteps: 198929 Episode num: 226 Reward: 699.3094761157192\n","Total Timesteps: 199929 Episode num: 227 Reward: 575.7501807416273\n","Total Timesteps: 200929 Episode num: 228 Reward: 446.55800635252683\n","----------------------------------------\n","Mean reward in the evaluation step: 493.566919\n","----------------------------------------\n","Total Timesteps: 201929 Episode num: 229 Reward: 452.0708505079896\n","Total Timesteps: 202929 Episode num: 230 Reward: 458.022106155293\n","Total Timesteps: 203929 Episode num: 231 Reward: 484.42849825449804\n","Total Timesteps: 204929 Episode num: 232 Reward: 552.3035879725811\n","Total Timesteps: 205929 Episode num: 233 Reward: 517.6324290575366\n","----------------------------------------\n","Mean reward in the evaluation step: 626.982371\n","----------------------------------------\n","Total Timesteps: 206929 Episode num: 234 Reward: 706.3303043790908\n","Total Timesteps: 207929 Episode num: 235 Reward: 671.8155871961862\n","Total Timesteps: 208929 Episode num: 236 Reward: 676.8382037290165\n","Total Timesteps: 209929 Episode num: 237 Reward: 727.3607214722623\n","Total Timesteps: 210929 Episode num: 238 Reward: 694.7964991833093\n","----------------------------------------\n","Mean reward in the evaluation step: 713.010408\n","----------------------------------------\n","Total Timesteps: 211929 Episode num: 239 Reward: 773.1405572385451\n","Total Timesteps: 212929 Episode num: 240 Reward: 541.334928652971\n","Total Timesteps: 213929 Episode num: 241 Reward: 666.5568217732442\n","Total Timesteps: 214929 Episode num: 242 Reward: 477.3308243791975\n","Total Timesteps: 215929 Episode num: 243 Reward: 569.7532539178862\n","----------------------------------------\n","Mean reward in the evaluation step: 609.309758\n","----------------------------------------\n","Total Timesteps: 216929 Episode num: 244 Reward: 674.7196461347829\n","Total Timesteps: 217929 Episode num: 245 Reward: 471.03835431026357\n","Total Timesteps: 218929 Episode num: 246 Reward: 650.1675233719708\n","Total Timesteps: 219929 Episode num: 247 Reward: 680.7012753129782\n","Total Timesteps: 220929 Episode num: 248 Reward: 651.936746191981\n","----------------------------------------\n","Mean reward in the evaluation step: 736.934601\n","----------------------------------------\n","Total Timesteps: 221929 Episode num: 249 Reward: 656.9465348759351\n","Total Timesteps: 222929 Episode num: 250 Reward: 826.2890676566619\n","Total Timesteps: 223929 Episode num: 251 Reward: 589.1043758536698\n","Total Timesteps: 224929 Episode num: 252 Reward: 754.6256468050169\n","Total Timesteps: 225929 Episode num: 253 Reward: 764.9047448979968\n","----------------------------------------\n","Mean reward in the evaluation step: 710.953146\n","----------------------------------------\n","Total Timesteps: 226929 Episode num: 254 Reward: 837.5770240639829\n","Total Timesteps: 227929 Episode num: 255 Reward: 373.07047565322944\n","Total Timesteps: 228929 Episode num: 256 Reward: 511.00092525657954\n","Total Timesteps: 229929 Episode num: 257 Reward: 498.53413162462994\n","Total Timesteps: 230929 Episode num: 258 Reward: 524.1015895132904\n","----------------------------------------\n","Mean reward in the evaluation step: 561.896504\n","----------------------------------------\n","Total Timesteps: 231422 Episode num: 259 Reward: 259.0681276629475\n","Total Timesteps: 232422 Episode num: 260 Reward: 553.4585338243313\n","Total Timesteps: 233422 Episode num: 261 Reward: 550.9110058479897\n","Total Timesteps: 234422 Episode num: 262 Reward: 800.371430378627\n","Total Timesteps: 235422 Episode num: 263 Reward: 957.89641237118\n","----------------------------------------\n","Mean reward in the evaluation step: 786.626953\n","----------------------------------------\n","Total Timesteps: 236422 Episode num: 264 Reward: 579.4417614121422\n","Total Timesteps: 237422 Episode num: 265 Reward: 661.5724103451262\n","Total Timesteps: 238422 Episode num: 266 Reward: 600.2562363176843\n","Total Timesteps: 239422 Episode num: 267 Reward: 715.0192802121203\n","Total Timesteps: 240422 Episode num: 268 Reward: 873.7903095398916\n","----------------------------------------\n","Mean reward in the evaluation step: 607.347772\n","----------------------------------------\n","Total Timesteps: 241422 Episode num: 269 Reward: 521.0012851279699\n","Total Timesteps: 242422 Episode num: 270 Reward: 654.6567095025657\n","Total Timesteps: 243422 Episode num: 271 Reward: 624.6501543875461\n","Total Timesteps: 244422 Episode num: 272 Reward: 460.97534417991454\n","Total Timesteps: 245422 Episode num: 273 Reward: 932.7335374571946\n","----------------------------------------\n","Mean reward in the evaluation step: 487.494113\n","----------------------------------------\n","Total Timesteps: 246422 Episode num: 274 Reward: 633.6859329443786\n","Total Timesteps: 247422 Episode num: 275 Reward: 659.481870236264\n","Total Timesteps: 248422 Episode num: 276 Reward: 691.8612030717228\n","Total Timesteps: 249422 Episode num: 277 Reward: 614.071043419882\n","Total Timesteps: 250422 Episode num: 278 Reward: 697.7930476666355\n","----------------------------------------\n","Mean reward in the evaluation step: 649.606202\n","----------------------------------------\n","Total Timesteps: 251422 Episode num: 279 Reward: 530.7099208543509\n","Total Timesteps: 252422 Episode num: 280 Reward: 713.7946081948115\n","Total Timesteps: 253422 Episode num: 281 Reward: 692.4815807878555\n","Total Timesteps: 254422 Episode num: 282 Reward: 653.106726169206\n","Total Timesteps: 255422 Episode num: 283 Reward: 639.0144436869597\n","----------------------------------------\n","Mean reward in the evaluation step: 751.413025\n","----------------------------------------\n","Total Timesteps: 256422 Episode num: 284 Reward: 824.0598856151621\n","Total Timesteps: 257422 Episode num: 285 Reward: 899.4003664010747\n","Total Timesteps: 258422 Episode num: 286 Reward: 791.9459983851979\n","Total Timesteps: 259422 Episode num: 287 Reward: 630.6780991888318\n","Total Timesteps: 260422 Episode num: 288 Reward: 914.2206807023838\n","----------------------------------------\n","Mean reward in the evaluation step: 764.947783\n","----------------------------------------\n","Total Timesteps: 261422 Episode num: 289 Reward: 924.4563223461198\n","Total Timesteps: 262422 Episode num: 290 Reward: 836.1156895924437\n","Total Timesteps: 263422 Episode num: 291 Reward: 630.7534481753785\n","Total Timesteps: 264422 Episode num: 292 Reward: 801.724706955649\n","Total Timesteps: 265422 Episode num: 293 Reward: 710.55212376848\n","----------------------------------------\n","Mean reward in the evaluation step: 734.488832\n","----------------------------------------\n","Total Timesteps: 266422 Episode num: 294 Reward: 751.3731981740129\n","Total Timesteps: 267422 Episode num: 295 Reward: 760.8545752589301\n","Total Timesteps: 268422 Episode num: 296 Reward: 781.2958617921114\n","Total Timesteps: 269422 Episode num: 297 Reward: 689.5758178781236\n","Total Timesteps: 270422 Episode num: 298 Reward: 657.7326913119523\n","----------------------------------------\n","Mean reward in the evaluation step: 698.857874\n","----------------------------------------\n","Total Timesteps: 271422 Episode num: 299 Reward: 758.1905489816448\n","Total Timesteps: 272422 Episode num: 300 Reward: 508.3755944908458\n","Total Timesteps: 273422 Episode num: 301 Reward: 611.6643914877479\n","Total Timesteps: 274422 Episode num: 302 Reward: 715.8431327599266\n","Total Timesteps: 275422 Episode num: 303 Reward: 772.677161421338\n","----------------------------------------\n","Mean reward in the evaluation step: 676.158995\n","----------------------------------------\n","Total Timesteps: 276422 Episode num: 304 Reward: 527.1787899516762\n","Total Timesteps: 277422 Episode num: 305 Reward: 802.3027296829027\n","Total Timesteps: 278422 Episode num: 306 Reward: 1087.181437071094\n","Total Timesteps: 279422 Episode num: 307 Reward: 1160.3618091555109\n","Total Timesteps: 280422 Episode num: 308 Reward: 792.2187019574133\n","----------------------------------------\n","Mean reward in the evaluation step: 790.661170\n","----------------------------------------\n","Total Timesteps: 281422 Episode num: 309 Reward: 290.31732672160865\n","Total Timesteps: 282422 Episode num: 310 Reward: 618.7813204552307\n","Total Timesteps: 283422 Episode num: 311 Reward: 937.66951367825\n","Total Timesteps: 284422 Episode num: 312 Reward: 1057.3607822063611\n","Total Timesteps: 285422 Episode num: 313 Reward: 465.5006723312139\n","----------------------------------------\n","Mean reward in the evaluation step: 787.281313\n","----------------------------------------\n","Total Timesteps: 286422 Episode num: 314 Reward: 924.2267233351322\n","Total Timesteps: 287422 Episode num: 315 Reward: 722.1175879160452\n","Total Timesteps: 288422 Episode num: 316 Reward: 751.8933992208028\n","Total Timesteps: 289422 Episode num: 317 Reward: 826.4275108926339\n","Total Timesteps: 290422 Episode num: 318 Reward: 810.687973283678\n","----------------------------------------\n","Mean reward in the evaluation step: 1136.181689\n","----------------------------------------\n","Total Timesteps: 291422 Episode num: 319 Reward: 884.078643943529\n","Total Timesteps: 292422 Episode num: 320 Reward: 1025.9869580133848\n","Total Timesteps: 293422 Episode num: 321 Reward: 873.2450031530111\n","Total Timesteps: 294422 Episode num: 322 Reward: 1119.2605657676718\n","Total Timesteps: 295422 Episode num: 323 Reward: 376.6170383065846\n","----------------------------------------\n","Mean reward in the evaluation step: 1093.978451\n","----------------------------------------\n","Total Timesteps: 296422 Episode num: 324 Reward: 1275.156017321912\n","Total Timesteps: 297422 Episode num: 325 Reward: 1253.4268181499929\n","Total Timesteps: 298422 Episode num: 326 Reward: 694.7679255841928\n","Total Timesteps: 299422 Episode num: 327 Reward: 1189.5213675807938\n","Total Timesteps: 300422 Episode num: 328 Reward: 588.0925255285898\n","----------------------------------------\n","Mean reward in the evaluation step: 1262.681461\n","----------------------------------------\n","Total Timesteps: 301422 Episode num: 329 Reward: 1177.8724348712924\n","Total Timesteps: 302422 Episode num: 330 Reward: 1170.6049885187654\n","Total Timesteps: 303422 Episode num: 331 Reward: 1190.3016730139018\n","Total Timesteps: 304422 Episode num: 332 Reward: 1183.8840550113578\n","Total Timesteps: 305422 Episode num: 333 Reward: 1146.4940538535895\n","----------------------------------------\n","Mean reward in the evaluation step: 1150.366065\n","----------------------------------------\n","Total Timesteps: 306422 Episode num: 334 Reward: 1172.0392976870764\n","Total Timesteps: 307422 Episode num: 335 Reward: 857.0435788253797\n","Total Timesteps: 308422 Episode num: 336 Reward: 1163.429930180969\n","Total Timesteps: 309422 Episode num: 337 Reward: 1188.8683846941858\n","Total Timesteps: 310422 Episode num: 338 Reward: 1151.504118060575\n","----------------------------------------\n","Mean reward in the evaluation step: 1074.788473\n","----------------------------------------\n","Total Timesteps: 311422 Episode num: 339 Reward: 778.4413085664645\n","Total Timesteps: 312422 Episode num: 340 Reward: 1247.4527320816812\n","Total Timesteps: 313422 Episode num: 341 Reward: 1252.897281573245\n","Total Timesteps: 314422 Episode num: 342 Reward: 1158.415552443003\n","Total Timesteps: 315422 Episode num: 343 Reward: 1365.4116424095037\n","----------------------------------------\n","Mean reward in the evaluation step: 1026.871347\n","----------------------------------------\n","Total Timesteps: 316422 Episode num: 344 Reward: 943.3860776178554\n","Total Timesteps: 317422 Episode num: 345 Reward: 676.0697558908804\n","Total Timesteps: 318422 Episode num: 346 Reward: 587.9186796313055\n","Total Timesteps: 319422 Episode num: 347 Reward: 1452.1746606640247\n","Total Timesteps: 320422 Episode num: 348 Reward: 1375.018779431729\n","----------------------------------------\n","Mean reward in the evaluation step: 1148.126524\n","----------------------------------------\n","Total Timesteps: 321422 Episode num: 349 Reward: 1648.4662979344773\n","Total Timesteps: 322422 Episode num: 350 Reward: 950.1030393202005\n","Total Timesteps: 323422 Episode num: 351 Reward: 1341.780719545188\n","Total Timesteps: 324422 Episode num: 352 Reward: 1446.2330832626135\n","Total Timesteps: 325422 Episode num: 353 Reward: 1439.8399184696348\n","----------------------------------------\n","Mean reward in the evaluation step: 989.682553\n","----------------------------------------\n","Total Timesteps: 326422 Episode num: 354 Reward: 1235.6136605250122\n","Total Timesteps: 327422 Episode num: 355 Reward: 1243.391836429915\n","Total Timesteps: 328422 Episode num: 356 Reward: 1661.7963982936772\n","Total Timesteps: 329422 Episode num: 357 Reward: 1292.3319662317222\n","Total Timesteps: 330422 Episode num: 358 Reward: 1605.2810158778038\n","----------------------------------------\n","Mean reward in the evaluation step: 1614.353657\n","----------------------------------------\n","Total Timesteps: 331422 Episode num: 359 Reward: 1718.5860762249413\n","Total Timesteps: 332422 Episode num: 360 Reward: 1384.3976688808789\n","Total Timesteps: 333422 Episode num: 361 Reward: 1566.4599397098707\n","Total Timesteps: 334422 Episode num: 362 Reward: 649.0421091135981\n","Total Timesteps: 335422 Episode num: 363 Reward: 1671.4132501532927\n","----------------------------------------\n","Mean reward in the evaluation step: 1911.446424\n","----------------------------------------\n","Total Timesteps: 336422 Episode num: 364 Reward: 1866.9386817742777\n","Total Timesteps: 337422 Episode num: 365 Reward: 1641.0480827870526\n","Total Timesteps: 338422 Episode num: 366 Reward: 1832.9523164389032\n","Total Timesteps: 339422 Episode num: 367 Reward: 1572.5769233607657\n","Total Timesteps: 340422 Episode num: 368 Reward: 1784.9779726274344\n","----------------------------------------\n","Mean reward in the evaluation step: 1626.148935\n","----------------------------------------\n","Total Timesteps: 341422 Episode num: 369 Reward: 1794.6917833425623\n","Total Timesteps: 342422 Episode num: 370 Reward: 1727.0185414652406\n","Total Timesteps: 343422 Episode num: 371 Reward: 1677.5257091755327\n","Total Timesteps: 344422 Episode num: 372 Reward: 1682.2158981704438\n","Total Timesteps: 345422 Episode num: 373 Reward: 1858.948603737847\n","----------------------------------------\n","Mean reward in the evaluation step: 1420.370984\n","----------------------------------------\n","Total Timesteps: 346422 Episode num: 374 Reward: 1569.5993394516581\n","Total Timesteps: 347422 Episode num: 375 Reward: 1672.4440709377209\n","Total Timesteps: 348422 Episode num: 376 Reward: 1777.0310715833057\n","Total Timesteps: 349422 Episode num: 377 Reward: 1713.0425705439925\n","Total Timesteps: 350422 Episode num: 378 Reward: 1763.129508135517\n","----------------------------------------\n","Mean reward in the evaluation step: 1792.766315\n","----------------------------------------\n","Total Timesteps: 351422 Episode num: 379 Reward: 1700.6548395001748\n","Total Timesteps: 352422 Episode num: 380 Reward: 1742.6997589615569\n","Total Timesteps: 353422 Episode num: 381 Reward: 1890.3624460918015\n","Total Timesteps: 354422 Episode num: 382 Reward: 1788.037188215526\n","Total Timesteps: 355422 Episode num: 383 Reward: 1775.3324255489845\n","----------------------------------------\n","Mean reward in the evaluation step: 1884.774820\n","----------------------------------------\n","Total Timesteps: 356422 Episode num: 384 Reward: 1895.9339020666469\n","Total Timesteps: 357422 Episode num: 385 Reward: 1882.349526355987\n","Total Timesteps: 358422 Episode num: 386 Reward: 1924.930326782749\n","Total Timesteps: 359422 Episode num: 387 Reward: 1955.4074272658545\n","Total Timesteps: 360422 Episode num: 388 Reward: 1951.8363141399914\n","----------------------------------------\n","Mean reward in the evaluation step: 1974.970588\n","----------------------------------------\n","Total Timesteps: 361422 Episode num: 389 Reward: 1975.6142595495296\n","Total Timesteps: 362422 Episode num: 390 Reward: 1885.3559980400566\n","Total Timesteps: 363422 Episode num: 391 Reward: 1907.8253510549614\n","Total Timesteps: 364422 Episode num: 392 Reward: 1923.6515223490405\n","Total Timesteps: 365422 Episode num: 393 Reward: 1838.036498490902\n","----------------------------------------\n","Mean reward in the evaluation step: 2093.178058\n","----------------------------------------\n","Total Timesteps: 366422 Episode num: 394 Reward: 2060.7913816517726\n","Total Timesteps: 367422 Episode num: 395 Reward: 2017.260758304936\n","Total Timesteps: 368422 Episode num: 396 Reward: 1965.1936604740208\n","Total Timesteps: 369422 Episode num: 397 Reward: 1775.7681028787542\n","Total Timesteps: 370422 Episode num: 398 Reward: 1942.7284755518997\n","----------------------------------------\n","Mean reward in the evaluation step: 1995.480731\n","----------------------------------------\n","Total Timesteps: 371422 Episode num: 399 Reward: 2001.6572066220335\n","Total Timesteps: 372422 Episode num: 400 Reward: 2079.945502410183\n","Total Timesteps: 373422 Episode num: 401 Reward: 2096.108475045536\n","Total Timesteps: 374422 Episode num: 402 Reward: 1934.5439187366715\n","Total Timesteps: 375422 Episode num: 403 Reward: 1909.5980844469084\n","----------------------------------------\n","Mean reward in the evaluation step: 1944.652044\n","----------------------------------------\n","Total Timesteps: 376422 Episode num: 404 Reward: 1923.0462927431752\n","Total Timesteps: 377422 Episode num: 405 Reward: 2081.963582280904\n","Total Timesteps: 378422 Episode num: 406 Reward: 1971.5018225962597\n","Total Timesteps: 379422 Episode num: 407 Reward: 2018.8398506368967\n","Total Timesteps: 380422 Episode num: 408 Reward: 2140.388503434005\n","----------------------------------------\n","Mean reward in the evaluation step: 2137.845451\n","----------------------------------------\n","Total Timesteps: 381422 Episode num: 409 Reward: 2056.795406952047\n","Total Timesteps: 382422 Episode num: 410 Reward: 2160.518246292999\n","Total Timesteps: 383422 Episode num: 411 Reward: 2128.5438719933736\n","Total Timesteps: 384422 Episode num: 412 Reward: 2212.896161911352\n","Total Timesteps: 385422 Episode num: 413 Reward: 2190.200438098655\n","----------------------------------------\n","Mean reward in the evaluation step: 2191.764937\n","----------------------------------------\n","Total Timesteps: 386422 Episode num: 414 Reward: 2120.240050480064\n","Total Timesteps: 387422 Episode num: 415 Reward: 2174.2603941055713\n","Total Timesteps: 388422 Episode num: 416 Reward: 2090.1208773187564\n","Total Timesteps: 389422 Episode num: 417 Reward: 2105.610481943408\n","Total Timesteps: 390422 Episode num: 418 Reward: 2220.110444501129\n","----------------------------------------\n","Mean reward in the evaluation step: 2126.169854\n","----------------------------------------\n","Total Timesteps: 391422 Episode num: 419 Reward: 2090.1767737056753\n","Total Timesteps: 392422 Episode num: 420 Reward: 2184.637545088008\n","Total Timesteps: 393422 Episode num: 421 Reward: 2113.49773923693\n","Total Timesteps: 394422 Episode num: 422 Reward: 2306.348438641104\n","Total Timesteps: 395422 Episode num: 423 Reward: 2115.2074064853823\n","----------------------------------------\n","Mean reward in the evaluation step: 2370.786673\n","----------------------------------------\n","Total Timesteps: 396422 Episode num: 424 Reward: 2300.916396665598\n","Total Timesteps: 397422 Episode num: 425 Reward: 2223.8888969519385\n","Total Timesteps: 398422 Episode num: 426 Reward: 2225.2518139167323\n","Total Timesteps: 399422 Episode num: 427 Reward: 2182.4905546015984\n","Total Timesteps: 400422 Episode num: 428 Reward: 2201.3162029198215\n","----------------------------------------\n","Mean reward in the evaluation step: 2280.690211\n","----------------------------------------\n","Total Timesteps: 401422 Episode num: 429 Reward: 2264.277115949666\n","Total Timesteps: 402422 Episode num: 430 Reward: 2175.75708553823\n","Total Timesteps: 403422 Episode num: 431 Reward: 2215.101909081455\n","Total Timesteps: 404422 Episode num: 432 Reward: 2245.2589038601004\n","Total Timesteps: 405422 Episode num: 433 Reward: 2133.3530780689803\n","----------------------------------------\n","Mean reward in the evaluation step: 2215.723937\n","----------------------------------------\n","Total Timesteps: 406422 Episode num: 434 Reward: 2223.058138285428\n","Total Timesteps: 407422 Episode num: 435 Reward: 2290.730642505623\n","Total Timesteps: 408422 Episode num: 436 Reward: 2269.6834410126444\n","Total Timesteps: 409422 Episode num: 437 Reward: 2208.161792113156\n","Total Timesteps: 410422 Episode num: 438 Reward: 2205.9860591977886\n","----------------------------------------\n","Mean reward in the evaluation step: 2305.372932\n","----------------------------------------\n","Total Timesteps: 411422 Episode num: 439 Reward: 2254.653565465967\n","Total Timesteps: 412422 Episode num: 440 Reward: 2032.0588547998132\n","Total Timesteps: 413422 Episode num: 441 Reward: 2382.052834908149\n","Total Timesteps: 414422 Episode num: 442 Reward: 2295.6293123755804\n","Total Timesteps: 415422 Episode num: 443 Reward: 2245.9828543964622\n","----------------------------------------\n","Mean reward in the evaluation step: 2304.942486\n","----------------------------------------\n","Total Timesteps: 416422 Episode num: 444 Reward: 2307.439713825446\n","Total Timesteps: 417422 Episode num: 445 Reward: 2206.544718548576\n","Total Timesteps: 418422 Episode num: 446 Reward: 2401.3087441680373\n","Total Timesteps: 419422 Episode num: 447 Reward: 2351.115884021975\n","Total Timesteps: 420422 Episode num: 448 Reward: 2295.497413984369\n","----------------------------------------\n","Mean reward in the evaluation step: 2418.726267\n","----------------------------------------\n","Total Timesteps: 421422 Episode num: 449 Reward: 2445.145504663411\n","Total Timesteps: 422422 Episode num: 450 Reward: 2366.666811664416\n","Total Timesteps: 423422 Episode num: 451 Reward: 2377.1731409963863\n","Total Timesteps: 424422 Episode num: 452 Reward: 2406.363487518962\n","Total Timesteps: 425422 Episode num: 453 Reward: 2373.0905414393287\n","----------------------------------------\n","Mean reward in the evaluation step: 2421.246141\n","----------------------------------------\n","Total Timesteps: 426422 Episode num: 454 Reward: 2376.8772315130955\n","Total Timesteps: 427422 Episode num: 455 Reward: 2360.1856889080236\n","Total Timesteps: 428422 Episode num: 456 Reward: 2431.8942206151532\n","Total Timesteps: 429422 Episode num: 457 Reward: 2227.653778893808\n","Total Timesteps: 430422 Episode num: 458 Reward: 2356.0096637209404\n","----------------------------------------\n","Mean reward in the evaluation step: 2346.868014\n","----------------------------------------\n","Total Timesteps: 431422 Episode num: 459 Reward: 2322.7694589649423\n","Total Timesteps: 432422 Episode num: 460 Reward: 2312.2706362570398\n","Total Timesteps: 433422 Episode num: 461 Reward: 2350.9305422266357\n","Total Timesteps: 434422 Episode num: 462 Reward: 2439.379641465865\n","Total Timesteps: 435422 Episode num: 463 Reward: 2380.173447568912\n","----------------------------------------\n","Mean reward in the evaluation step: 2415.587919\n","----------------------------------------\n","Total Timesteps: 436422 Episode num: 464 Reward: 2364.048723195784\n","Total Timesteps: 437422 Episode num: 465 Reward: 2120.3107707784966\n","Total Timesteps: 438422 Episode num: 466 Reward: 2133.4897046683463\n","Total Timesteps: 439422 Episode num: 467 Reward: 2479.246745419602\n","Total Timesteps: 440422 Episode num: 468 Reward: 2448.20502766401\n","----------------------------------------\n","Mean reward in the evaluation step: 2366.552589\n","----------------------------------------\n","Total Timesteps: 441422 Episode num: 469 Reward: 2310.3456897847223\n","Total Timesteps: 442422 Episode num: 470 Reward: 2490.1629645045064\n","Total Timesteps: 443422 Episode num: 471 Reward: 2397.0227685083646\n","Total Timesteps: 444422 Episode num: 472 Reward: 2304.1450596023124\n","Total Timesteps: 445422 Episode num: 473 Reward: 2433.2696484298194\n","----------------------------------------\n","Mean reward in the evaluation step: 2505.222212\n","----------------------------------------\n","Total Timesteps: 446422 Episode num: 474 Reward: 2484.5067880318074\n","Total Timesteps: 447422 Episode num: 475 Reward: 2506.551486052477\n","Total Timesteps: 448422 Episode num: 476 Reward: 2443.5770055952553\n","Total Timesteps: 449422 Episode num: 477 Reward: 2337.320420518564\n","Total Timesteps: 450422 Episode num: 478 Reward: 2287.78205294956\n","----------------------------------------\n","Mean reward in the evaluation step: 2310.442259\n","----------------------------------------\n","Total Timesteps: 451422 Episode num: 479 Reward: 2265.6330395272294\n","Total Timesteps: 452422 Episode num: 480 Reward: 2481.866980447046\n","Total Timesteps: 453422 Episode num: 481 Reward: 2306.678608196673\n","Total Timesteps: 454422 Episode num: 482 Reward: 2010.3120255572196\n","Total Timesteps: 455422 Episode num: 483 Reward: 2501.158489087347\n","----------------------------------------\n","Mean reward in the evaluation step: 2455.533280\n","----------------------------------------\n","Total Timesteps: 456422 Episode num: 484 Reward: 2417.9290520047366\n","Total Timesteps: 457422 Episode num: 485 Reward: 2415.187881316517\n","Total Timesteps: 458422 Episode num: 486 Reward: 1215.8339574522006\n","Total Timesteps: 459422 Episode num: 487 Reward: 2451.0472977898808\n","Total Timesteps: 460422 Episode num: 488 Reward: 2391.320288371735\n","----------------------------------------\n","Mean reward in the evaluation step: 1969.696119\n","----------------------------------------\n","Total Timesteps: 461422 Episode num: 489 Reward: 2141.2588679677665\n","Total Timesteps: 462422 Episode num: 490 Reward: 2406.880520722232\n","Total Timesteps: 463422 Episode num: 491 Reward: 2459.760666340835\n","Total Timesteps: 464422 Episode num: 492 Reward: 2485.327830927416\n","Total Timesteps: 465422 Episode num: 493 Reward: 2449.5671832250537\n","----------------------------------------\n","Mean reward in the evaluation step: 2400.617593\n","----------------------------------------\n","Total Timesteps: 466422 Episode num: 494 Reward: 2301.9286418284655\n","Total Timesteps: 467422 Episode num: 495 Reward: 2514.5621664043633\n","Total Timesteps: 468422 Episode num: 496 Reward: 2560.9029188137447\n","Total Timesteps: 469422 Episode num: 497 Reward: 2491.6159140363675\n","Total Timesteps: 470422 Episode num: 498 Reward: 2463.5493101768557\n","----------------------------------------\n","Mean reward in the evaluation step: 2508.253565\n","----------------------------------------\n","Total Timesteps: 471422 Episode num: 499 Reward: 2497.655704290194\n","Total Timesteps: 472422 Episode num: 500 Reward: 2526.5661718448027\n","Total Timesteps: 473422 Episode num: 501 Reward: 2400.402857104812\n","Total Timesteps: 474422 Episode num: 502 Reward: 2472.1412543533575\n","Total Timesteps: 475422 Episode num: 503 Reward: 2367.462158444629\n","----------------------------------------\n","Mean reward in the evaluation step: 2522.272116\n","----------------------------------------\n","Total Timesteps: 476422 Episode num: 504 Reward: 2534.5902495279825\n","Total Timesteps: 477422 Episode num: 505 Reward: 2610.8396725379357\n","Total Timesteps: 478422 Episode num: 506 Reward: 2458.870081461251\n","Total Timesteps: 479422 Episode num: 507 Reward: 2445.8882446993293\n","Total Timesteps: 480422 Episode num: 508 Reward: 2499.4719931198315\n","----------------------------------------\n","Mean reward in the evaluation step: 2528.882936\n","----------------------------------------\n","Total Timesteps: 481422 Episode num: 509 Reward: 2489.534483999829\n","Total Timesteps: 482422 Episode num: 510 Reward: 2417.1808656714798\n","Total Timesteps: 483422 Episode num: 511 Reward: 2485.6946015094277\n","Total Timesteps: 484422 Episode num: 512 Reward: 2446.0891243102164\n","Total Timesteps: 485422 Episode num: 513 Reward: 2314.229278045894\n","----------------------------------------\n","Mean reward in the evaluation step: 2508.123933\n","----------------------------------------\n","Total Timesteps: 486422 Episode num: 514 Reward: 2475.278471639638\n","Total Timesteps: 487422 Episode num: 515 Reward: 2476.5212742597587\n","Total Timesteps: 488422 Episode num: 516 Reward: 2374.877336145124\n","Total Timesteps: 489422 Episode num: 517 Reward: 2410.888945221729\n","Total Timesteps: 490422 Episode num: 518 Reward: 2394.449676836854\n","----------------------------------------\n","Mean reward in the evaluation step: 2444.913181\n","----------------------------------------\n","Total Timesteps: 491422 Episode num: 519 Reward: 2448.001099225968\n","Total Timesteps: 492422 Episode num: 520 Reward: 2258.6918068895307\n","Total Timesteps: 493422 Episode num: 521 Reward: 2567.969698139013\n","Total Timesteps: 494422 Episode num: 522 Reward: 2460.484607529143\n","Total Timesteps: 495422 Episode num: 523 Reward: 2523.175797351104\n","----------------------------------------\n","Mean reward in the evaluation step: 2055.610932\n","----------------------------------------\n","Total Timesteps: 496422 Episode num: 524 Reward: 1953.1974433507608\n","Total Timesteps: 497422 Episode num: 525 Reward: 2505.0052524632288\n","Total Timesteps: 498422 Episode num: 526 Reward: 2496.335513234775\n","Total Timesteps: 499422 Episode num: 527 Reward: 2378.8875861222773\n","----------------------------------------\n","Mean reward in the evaluation step: 2494.922114\n","----------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QqUWscDWG8vk"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"mwTYnpP2G_DU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611391346179,"user_tz":-60,"elapsed":81352,"user":{"displayName":"Fran Vaquer","photoUrl":"","userId":"05040906435658654965"}},"outputId":"bc7048d3-6395-4f76-ab72-14450fb29844"},"source":["class Actor(nn.Module):\n","  def __init__(self, state_dim, action_dim, max_action):\n","    super(Actor, self).__init__()\n","    self.layer_1 = nn.Linear(in_features= state_dim, out_features= 400)\n","    self.layer_2 = nn.Linear(400, 300)\n","    self.layer_3 = nn.Linear(300, action_dim)\n","    self.max_action = max_action\n","\n","  def forward(self, x):\n","    x = F.relu(self.layer_1(x))\n","    x = F.relu(self.layer_2(x))\n","    x = self.max_action * torch.tanh(self.layer_3(x))\n","    return  x\n","\n","class Critic(nn.Module):\n","  def __init__(self, state_dim, action_dim):\n","    super(Critic, self).__init__()\n","\n","    #Define twin 1 as a deep neural network\n","    self.layer_1 = nn.Linear(in_features= state_dim + action_dim, out_features= 400)\n","    self.layer_2 = nn.Linear(400, 300)\n","    self.layer_3 = nn.Linear(300, 1)\n","\n","    #Define twin 2 as a deep neural network\n","    self.layer_4 = nn.Linear(in_features= state_dim + action_dim, out_features= 400)\n","    self.layer_5 = nn.Linear(400, 300)\n","    self.layer_6 = nn.Linear(300, 1)\n","\n","  def forward(self, x, u):\n","    xu = torch.cat([x,u], 1) #Vertical concat\n","\n","    #Forward propagation of first critic\n","    x1 = F.relu(self.layer_1(xu))\n","    x1 = F.relu(self.layer_2(x1))\n","    x1 = self.layer_3(x1)\n","\n","    #Forward propagation of second critic\n","    x2 = F.relu(self.layer_4(xu))\n","    x2 = F.relu(self.layer_5(x2))\n","    x2 = self.layer_6(x2)\n","\n","    return  x1, x2\n","\n","#Select GPU or CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","#Build training process in one class\n","\n","class TD3(object):\n","  def __init__(self, state_dim, action_dim, max_action):\n","    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n","    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n","    self.actor_target.load_state_dict(self.actor.state_dict())\n","    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n","    self.critic = Critic(state_dim, action_dim).to(device)\n","    self.critic_target = Critic(state_dim, action_dim).to(device)\n","    self.critic_target.load_state_dict(self.critic.state_dict())\n","    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n","    self.max_action = max_action\n","\n","  def select_action(self, state):\n","    state = torch.Tensor(state.reshape(1, -1)).to(device)\n","    return self.actor(state).cpu().data.numpy().flatten()\n","\n","  def train(self, replay_buffer, iterations, bach_size = 100, discount = 0.99, tau = 0.005, policy_noise = 0.2, noise_clipping = 0.5, policy_freq = 2):\n","    for it in range(iterations):\n","      #Take a sample from state, next state, action and reward of the memory. \n","      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = ReplayBuffer.sample(batch_size)\n","      state = torch.Tensor(batch_states).to(device)\n","      next_state = torch.Tensor(batch_next_states).to(device)\n","      action = torch.Tensor(batch_actions).to(device)\n","      reward = torch.Tensor(batch_rewards).to(device)\n","      done = torch.Tensor(batch_dones).to(device)\n","      #For each element of sample:\n","      #from next state, the actor target execute next action\n","      next_action = self.actor_target(next_state).to(device)\n","      #Add gaussian noise to the next action and cut to have in range of accepted environment\n","      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n","      noise = noise.clamp(-noise_clipping, noise_clipping)\n","      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n","      #both target critics take next action and next state for inputs and return two Q-values for outputs\n","      target_Q1, target_Q2 = self.critic_target(next_state, next_action).to(device)\n","      #take the minimum of the values that are obtained before\n","      target_Q = torch.min(target_Q1, target_Q2)\n","      #Get the final Q target from the two critic models Q = r + y* min(Q11,Q12), where y is a discount factor\n","      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n","      #Two critics take state and action for inputs and return two Q-values Q1 and Q2 for output\n","      current_Q1, current_Q2 = self.critic(state, action)\n","      #Calculate loss from model critic: Critic_loss = MSE_Loss(Q1(s, a), Qt) + MSE_Loss(Q2(s, a), Qt)\n","      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n","      #Back propagation of the mse_loss and upgrade the two parameters of critic model with SGD\n","      self.critic_optimizer.zero_grad()\n","      critic_loss.backward()\n","      self.critic_optimizer.step()\n","      #To each of two iterations, upgrade the actor model by play the ascendence gradient at the first model critic output\n","      if it%policy_freq == 0:\n","        actor_loss = - self.critic(state, self.actor(state))[0].mean() #Acendence gradient from negative descendence gradient / REVISAR PASO 15\n","        self.actor_optimizer.zero_grad()\n","        actor_optimizer.backward()\n","        self.actor_optimizer.step()\n","        #Upgrade the weights of the target actor using polyak model\n","        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n","          target_param.data.copy_(tau * param + (1 - tau) * target_param)\n","        #Upgrade the weights of the target critic using polyak model\n","        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n","          target_param.data.copy_(tau * param + (1 - tau) * target_param)\n","    #Method to save the trained model\n","  def save(self, filename, directory):\n","      torch.save(self.actor.state_dict(), \"%s/%s_actor.pth\" % (directory, filename))\n","      torch.save(self.critic.state_dict(), \"%s/%s_critic.pth\" % (directory, filename))\n","    #Method to load the trained model\n","  def load(self, filename, directory):\n","      self.actor.load_state_dict(torch.load(\"%s/%s_actor.pth\" % (directory, filename)))\n","      self.critic.load_state_dict(torch.load(\"%s/%s_critic.pth\" % (directory, filename)))\n","\n","def evaluate_policy(policy, eval_episodes=10):\n","  avg_reward = 0.\n","  for _ in range(eval_episodes):\n","    obs = env.reset()\n","    done = False\n","    while not done:\n","      action = policy.select_action(np.array(obs))\n","      obs, reward, done, _ = env.step(action)\n","      avg_reward += reward\n","  avg_reward /= eval_episodes\n","  print(\"----------------------------------------\")\n","  print(\"Mean reward in the evaluation step: %f\" % (avg_reward))\n","  print(\"----------------------------------------\")\n","  return avg_reward\n","\n","env_name = \"AntBulletEnv-v0\" # Environment name\n","seed = 0 #Value of random seed\n","\n","file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n","print(\"----------------------------------------\")\n","print(\"Configuration: %s\" % (file_name))\n","print(\"----------------------------------------\")\n","\n","eval_episodes = 10\n","save_env_vid = True\n","env = gym.make(env_name)\n","max_episode_steps = env._max_episode_steps\n","if save_env_vid:\n","  env = wrappers.Monitor(env, monitor_dir, force = True)\n","  env.reset()\n","env.seed(seed)\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.shape[0]\n","max_action = float(env.action_space.high[0])\n","\n","policy = TD3(state_dim, action_dim, max_action)\n","policy.load(file_name, './pytorch_models/')\n","_ = evaluate_policy(policy, eval_episodes=eval_episodes)\n"],"execution_count":21,"outputs":[{"output_type":"stream","text":["----------------------------------------\n","Configuration: TD3_AntBulletEnv-v0_0\n","----------------------------------------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"],"name":"stderr"},{"output_type":"stream","text":["----------------------------------------\n","Mean reward in the evaluation step: 2487.631804\n","----------------------------------------\n"],"name":"stdout"}]}]}